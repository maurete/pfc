function [svm_params,out] = select_model_trivial(problem, feats, kernel, ...
                                                 lib,svm_tol)
%SELECT_MODEL_TRIVIAL Trivially select SVM model parameters
%
%  [SVM_PARAMS,MODEL] = SELECT_MODEL_TRIVIAL(PROBLEM,FEATS,KERNEL,LIB,SVM_TOL)
%  performs SVM model selection by trivially selecting model parameters.
%  Trivial parameters are log(C)=0 and log(gamma)=-log(2*feature vector length)
%  as proposed in Glasmachers & Igel, "Maximum Likelihood Model Selection for
%  1-Norm Soft Margin SVMs with Multiple Parameters" (YYYY).
%  Input arguments, (*)=optional:
%    PROBLEM the problem with trained data generated by PROBLEM_GEN
%    FEATS the featureset index, commonly 5 or 8
%    KERNEL a string with value 'linear' for linear kernel or 'rbf' for RBF
%    LIB the SVM library to be used, either 'libsvm' or 'matlab'
%    SVM_TOL (*) precision for SVM training (default=1e-6)
%  Output arguments:
%    SVM_PARAMS the optimal SVM parameters found
%    MODEL the optimal trained model
%
%  See also PROBLEM_GEN, SELECT_MODEL.
%

    if nargin < 5 || isempty(svm_tol), svm_tol = 1e-6; end

    kernel = get_kernel(kernel);
    features = featset_index(feats);

    % initial (and definite) parameter vector
    svm_params = 0;
    if get_kernel(kernel,'rbf',false)
        svm_params = [svm_params, -log(2*numel(features))];
    end

    % Build trained output model
    out = struct();
    out.features = features;
    out.trainfunc = func2str(@(in,tg) mysvm_train( lib, kernel, in, tg, ...
            exp(svm_params(1)), exp(svm_params(2:end)), false, svm_tol ));
    out.trainfuncargs = struct();
    out.trainfuncargs.lib = lib;
    out.trainfuncargs.kernel = kernel;
    out.trainfuncargs.svm_params = svm_params;
    out.trainfuncargs.svm_tol = svm_tol;
    out.classfunc = 'mysvm_classify';
    out.trainedmodel = mysvm_train( ...
        lib, kernel, problem.traindata(:,features), problem.trainlabels, ...
        exp(svm_params(1)), exp(svm_params(2:end)), false, svm_tol );

end
