function [best,out,hid,res,names,nt] = select_model_mlp(...
    problem, feats, criterion, method, repeat, disp, fann, trivial)
%SELECT_MODEL_MLP Simple MLP model selection by exhaustive search.
%
%  [BEST,MODEL,HID,RES,NAMES] = SELECT_MODEL_MLP(PROBLEM, FEATS, CRITERION, ...
%      METHOD, REPEAT, DISP, FANN) performs model selection for an MLP
%  classifier by evaluating performance for different number of neurons in the
%  hidden layer.
%  Input arguments, (*)=optional:
%    PROBLEM the problem with trained data generated by PROBLEM_GEN
%    FEATS the featureset index, commonly 5 or 8
%    CRITERION (*) the performance criterion, any of 'se' (sensitivity/recall),
%    'sp' (specificity), 'gm' (geometric mean of se and sp, default), 'emp'
%    (empirical risk), 'nll' (negative log lokelihood), or 'mcc' (Matthews'
%    Correlation Coefficient)
%    METHOD (*) the back-propagation method to be used (default='trainrp')
%    REPEAT (*) number of repeat training for averaging
%    DISP (*) wether to plot performance graphics (default=true)
%    FANN (*) wether to use the FANN library functions (default=false)
%  Output arguments:
%    BEST the optimal number of neurons in the hidden layer
%    MODEL the optimal trained MLP model
%    HID the number of hidden layers tried
%    RES performance measures for every element of HID
%    NAMES name for each measure in RES
%    NT number of trainings
%
%  See also PROBLEM_GEN, SELECT_MODEL, MLP_XTRAIN.
%

    if nargin < 8 || isempty(trivial),     trivial = false;     end
    if nargin < 7 || isempty(fann),           fann = false;     end
    if nargin < 6 || isempty(disp),           disp = true;      end
    if nargin < 5 || isempty(repeat),       repeat = 1;         end
    if nargin < 4 || isempty(method),       method = 'trainrp'; end
    if nargin < 3 || isempty(criterion), criterion = 'gm';      end

    % error names
    names = { 'se'      , ... % sensitivity
              'sp'      , ... % specificity
              'gm'      , ... % gm
              'emp'     , ... % empirical error (Adankon et al.)
              'nll'     , ... % negative log likelihood (Glasmachers & Igel; Keerthi et al.)
              'mcc'       ... % Matthews Correlation Coefficient
            };

    % name indexes
    ni = struct();
    for i=1:length(names), ni.(names{i}) = i; end

    features = featset_index(feats);

    % number of neurons in hidden layer
    % nh = 0:max([10, round(numel(features)*0.7)]);
    nh = [ 0:2, round(logspace(log10(3), log10(200), 17)) ];
    % nh = [0:9, 10:2:20, 25:5:40, 50:10:100, 150:20:250];

    time = time_init();
    time = time_tick(time, 1);

    % sign of the objective function
    crit_sgn = 1;
    crit_idx = ni.(criterion);
    if any(strcmpi(criterion, {'emp', 'nll', 'rmb'}))
        crit_sgn = -1;
    end

    if trivial
        nh = 0;
        best = 0;
        disp = 0;
    end

    % results matrix
    res = nan(length(nh),length(names));

    if ~trivial
        % training, testing and error functions
        trainfunc = @(in,tg,vi,vt,th) mlp_xtrain(in,tg,vi,vt,th,method,[],fann);
        testfunc  = @mlp_classify;
        eerrfunc  = @(out,trg) log(sum(error_empirical(out,trg)));
        enllfunc  = @(out,trg) log(error_nll(...
            @model_sigmoid, [], model_sigmoid_train(out,trg),out,trg));

        % main loop
        for k = 1:length(nh)

            theta = [nh(k)];

            % classification outputs
            out = nan(sum(problem.partitions.validation(:)), repeat);

            % repeat cross-validation training r times
            for r = 1:repeat
                [out(:,r),tar,~,~,s] = cross_validation( ...
                    problem, features, trainfunc, theta, testfunc,[],true);
                stat(r)=s;
            end

            % avoid computing following values for speed
            emp = nan;%eerrfunc(mean(out,2),tar);
            nll = nan;%enllfunc(mean(out,2),tar);

            % statistical performance measures
            res(k,ni.se) = mean([stat.se]);
            res(k,ni.sp) = mean([stat.sp]);
            res(k,ni.gm) = mean([stat.gm]);
            res(k,ni.mcc) = mean([stat.mc]);
            res(k,ni.emp) = emp;
            res(k,ni.nll) = nll;

            if mod(k,10) == 0, fprintf('|'), else fprintf('.'), end

        end % for k

        % find number of hidden layers with best erformance
        [ii] = find(abs(crit_sgn.*res(:,crit_idx) ...
                        -max(crit_sgn.*res(:,crit_idx)))<1e-5,1,'first');
        best = nh(ii)

    end

    time = time_tick(time, 1);

    % plot performance measures
    if disp
        figure
        hold all
        h = [];
        l = {};
        for i=1:length(names)
            h = [h plot(nh, res(:,i))];
        end
        legend(h,names)
        xlabel( '#hidden' )
        hold off
    end

    time_tick(time,0);

    hid = nh;

    % Build trained output model
    out = struct();
    out.features = features;
    out.trainfunc = func2str(...
        @(in,tg) mlp_xtrain(in,tg,[],[],best,method,[],fann));
    out.trainfuncargs = struct();
    out.trainfuncargs.best = best;
    out.trainfuncargs.method = method;
    out.trainfuncargs.fann = fann;
    out.classfunc = 'mlp_classify';
    out.trainedmodel = {};
    for i=1:repeat
        out.trainedmodel{i} = mlp_xtrain(problem.traindata(:,features), ...
                                         problem.trainlabels,[],[],best,...
                                         method,[],fann);
    end

    nt = (~trivial)*numel(nh)*repeat;

end
